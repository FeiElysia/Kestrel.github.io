<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Kestrel</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Mahmoud_Ahmed2" target="_blank">Mahmoud Ahmed</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://feielysia.github.io/" target="_blank">Junjie Fei</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://dingjiansw101.github.io/" target="_blank">Jian Ding</a>,</span>
                    <span class="author-block">
                      <a href="https://openreview.net/profile?id=~Eslam_Mohamed_BAKR1" target="_blank">Eslam Mohamed BAKR</a>,</span>
                      <span class="author-block">
                        <a href="https://www.mohamed-elhoseiny.com/" target="_blank">Mohamed Elhoseiny</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Vision CAIR, King Abdullah University of Science and Technology<br>ICCV 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block"> -->
                        <!-- <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" -->
                        <!-- class="external-link button is-normal is-rounded is-dark"> -->
                        <!-- <span class="icon"> -->
                          <!-- <i class="fas fa-file-pdf"></i> -->
                        <!-- </span> -->
                        <!-- <span>Paper</span> -->
                      <!-- </a> -->
                    <!-- </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block"> -->
                      <!-- <a href="static/pdfs/supplementary_material.pdf" target="_blank" -->
                      <!-- class="external-link button is-normal is-rounded is-dark"> -->
                      <!-- <span class="icon"> -->
                        <!-- <i class="fas fa-file-pdf"></i> -->
                      <!-- </span> -->
                      <!-- <span>Supplementary</span> -->
                    <!-- </a> -->
                  <!-- </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2405.18937" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (To be released)</span>
                  </a>
                </span>

                <!-- HuggingFace link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/YOUR_DATASET_HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-hugging-face"></i>
                    </span>
                    <span>3DCoMPaT-GRIN (To be released)</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
      <img src="static/images/teaser.svg" alt="MY ALT TEXT"/>
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- <b>Grounded 3D Descriptions with Kestrel.</b> Kestrel is a part-aware point grounding multimodal large language model (MLLM) capable of comprehending natural language and grounding the position of an object's parts and materials. (a) Kestrel responds to user instruction accurately even at the part level, an ability that noe of the previous 3D MLLMs possess. (b) Kestrel can generate detailed descriptions and grounding object parts mentioned in the response. (c) Kestrel enables dialogue and reasoning over part-level information. -->
      <!-- </h2> -->
      <div class="content has-text-justified">
        <p>
          <b>Part-Aware Point Grounded Description.</b> Given an input point cloud, the model is tasked with predicting a grounded description - text that provides a detailed interpretation of the 3D object, each part-level phrase in this generated text (<i>e.g.,</i> “back-rest” and “seat support”) is linked to a point-wise segmentation mask, challenging the model's capability for part-aware language understanding and segmentation grounding (it is worth noting that the colors shown in this figure are not the actual colors of the point cloud but are used to represent the segmentation mask).
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While 3D multimodal large language models (MLLMs) have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel: a part-aware point grounding MLLM, representing a novel approach that empowers 3D MLLMs with part-aware understanding, enabling better interpretation and segmentation grounding of 3D objects at the part level. Despite its significance, the current landscape lacks tasks and datasets that endow and assess the part-aware understanding ability of 3D MLLMs. To address this, we propose two novel tasks: <i>Part-Aware Point Grounding</i> and <i>Part-Aware Point Grounded Captioning</i>. In Part-Aware Point Grounding, the model is tasked with directly predicting a part-level segmentation mask based on user instructions. In Part-Aware Point Grounded Captioning, the model provides a detailed caption that includes part-level descriptions, where each part-level description in the answer corresponds to a segmentation mask. To support learning and evaluating for the proposed tasks, we introduce two versions of <i>3DCoMPaT Grounded Instructions Dataset</i> (3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point cloud-instruction-segmentation mask triplets, is used to evaluate MLLMs' ability of part-aware segmentation grounding based on user instructions. 3DCoMPaT-GRIN Grounded Caption, containing 107k part-aware point cloud-instruction-grounded caption triplets, assesses both MLLMs' part-aware language comprehension and segmentation grounding capabilities. Our introduced tasks, dataset, and Kestrel represent a preliminary effort to bridge the gap between human cognition and 3D MLLMs, <i>i.e.,</i> the ability to perceive and engage with the environment at both global and part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel can accurately generate user-specific segmentation masks, a capability not present in any existing 3D MLLMs. Kestrel thus established a benchmark for evaluating the part-aware language comprehension and segmentation grounding of 3D objects.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Model -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- Paper model. -->
          <h2 class="title is-3">Model</h2>
          <div class="content has-text-justified">
            <p>
              Kestrel includes both 3D vision-language modules and 3D segmentation grounding modules. Vision-language module <math><msub><mi>f</mi><mn>VL</mn></msub></math> projects the input cloud and text instruction into language hidden states. Decoding these language hidden states, we can get a detailed caption with part-level description. Each grounded part-level description (<i>e.g.,</i> <span style="color:rgb(181, 60, 181);">backrest</span>, <span style="color:rgb(82, 195, 64);">legs</span>, ...) in the answer can extract a <em>[SEG]</em> token, the projection layer <math><msub><mi>f</mi><mn>P</mn></msub></math> maps the hidden states of <em>[SEG]</em> tokens to the queries of segmentation grounding decoder <math><msub><mi>f</mi><mn>D</mn></msub></math>. Meanwhile, the segmentation grounding decoder also takes the point features, extracted by the segmentation grounding encoder <math><msub><mi>f</mi><mn>E</mn></msub></math>, as input and predicts the corresponding masks indicating by <em>[SEG]</em> tokens.
            </p>
          </div>
          <img src="static/images/arch.jpg" alt="MY ALT TEXT"/>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Model -->

<!-- Dataset -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">3DCoMPaT-GRIN</h2>
          <div class="content has-text-justified">
            <p>
                Here, we provide some examples of 3DCoMPaT-GRIN Grounded Caption. The label of this dataset is grounded caption, a multimodal caption comprising of detailed description and segmentation masks (part and material masks). Positional tokens <i>&lt;p&gt;</i> and <i>&lt;/p&gt;</i> refer to the part-level information that needs to be grounded in the caption.
            </p>
          </div>
          <img src="static/images/dataset.jpg" alt="MY ALT TEXT"/>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End dataset -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Demos</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/demo1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Part-Aware Point Grounding</b>. Kestrel is capable of grounding user-specified part in an 3D object using voxel-wise segmentation mask.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/demo2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Part-Aware Point Grounded Caption</b>. Kestrel can generate multimodal grounded caption, a detailed description with voxel-wise segmentation mask corresponding to part-level information in the response.
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2> -->
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <!-- </div> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <h2 class="title is-3">Another Carousel</h2> -->
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <!-- <div class="item item-video1"> -->
          <!-- <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video2"> -->
          <!-- <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video3"> -->
          <!-- <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4" -->
            <!-- type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <h2 class="title">Poster</h2> -->

      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550"> -->
          <!-- </iframe> -->
        
      <!-- </div> -->
    <!-- </div> -->
  <!-- </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{fei2024kestrel,
          title={Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding},
          author={Fei, Junjie and Ahmed, Mahmoud and Ding, Jian and Bakr, Eslam Mohamed and Elhoseiny, Mohamed},
          journal={arXiv preprint arXiv:2405.18937},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
